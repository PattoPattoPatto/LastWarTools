# -*- coding: utf-8 -*-
"""ocr

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H4oJ4Sl3ub6S6ms0-VNC_8qeS7egGMqi
"""

# Cell ID: z8ugEM8_Or1u

"""First, we need to install the `pytesseract` library and Tesseract OCR engine, along with the language data files for the languages you need."""

# Cell ID: 78efcc2e
!apt-get update
!apt-get install tesseract-ocr libtesseract-dev tesseract-ocr-eng tesseract-ocr-chi-sim tesseract-ocr-kor tesseract-ocr-jpn
!pip install pytesseract Pillow

"""Now, you can use `pytesseract` to perform OCR. You'll need to upload your image to Colab or access it from a cloud storage service.

Here's an example using a placeholder for your image file:
"""

# Cell ID: fa2280d4
import pytesseract
from PIL import Image

# Replace 'your_image.png' with the path to your image file
image_path = 's27NameOnly.png' # Updated to the user's cropped image filename

try:
    img = Image.open(image_path)

    # Specify the languages to use (English, Simplified Chinese, Korean, Japanese)
    # 'eng', 'chi-sim', 'kor', 'jpn' are the Tesseract language codes
    languages = 'eng+chi-sim+kor+jpn'

    # Specify Tesseract configuration options
    # '--psm 3' sets the page segmentation mode to automatic page segmentation (default)
    # This is generally good for pages with text blocks and lines that are not a single uniform block
    tessdata_dir_config = r'--psm 3'


    # Perform OCR
    text = pytesseract.image_to_string(img, lang=languages, config=tessdata_dir_config)

    print("Extracted Text:")
    print(text)

except FileNotFoundError:
    print(f"Error: Image file not found at {image_path}")
except Exception as e:
    print(f"An error occurred: {e}")

"""[Rac] Riz Maui

Warzone #27

[lolz] 베일 lolz

Warzone #27

[lolz] 도사 lolz

Warzone #27

[x3y] 空降

Warzone #27

[lolz] 폭주핑 lolz

Warzone #27

[Rac] Nada L

Warzone #27

[lolz] 우렁이 lolz

Warzone #27

**To use this code:**

1.  **Upload your image:** In the Colab file browser (folder icon on the left), upload the image file you want to process.
2.  **Update `image_path`:** Change `'your_image.png'` in the code to the actual name of your uploaded image file.
3.  **Run the cells:** Execute the code cells sequentially.

This will print the extracted text from your image. Remember that the accuracy can still depend on the image quality and the clarity of the text.

[Rac] Riz Maui

[lolz] 베일 lolz

[lolz] 도사 lolz

[x3y] 空降

[lolz] 폭주핑 lolz

[Rac] Nada L

[lolz] 우렁이 lolz
"""

# Cell ID: f2472ca5
# Assuming 'text' variable contains the output from the previous OCR cell (fa2280d4)

# Split the extracted text into lines
lines = text.strip().split('\n')

# Select every other line, starting from the first line (index 0)
# Adjust the starting index or step if the pattern is different
desired_lines = lines[0::2]

# Join the selected lines back into a single string or process them further
processed_text = '\n'.join(desired_lines)

print("Processed Text (Every Other Line):")
print(processed_text)

"""**Note:** This code assumes a consistent pattern where the desired information is on the first, third, fifth, etc., lines and the unwanted "Warzone" information is on the second, fourth, sixth, etc., lines. If the pattern is different, you might need to adjust the slicing `lines[0::2]`.

Based on the image description, we can use color thresholding to isolate the black text (clan tag and username) from the rest of the image before performing OCR.
"""

# Cell ID: fce5e8a3
import pytesseract
from PIL import Image, ImageOps

# Replace 'your_image.png' with the path to your image file
image_path = 's27NameOnly.png' # Make sure this is the correct path to your image

try:
    img = Image.open(image_path).convert('RGB') # Ensure image is in RGB for color processing

    # Convert the image to grayscale
    gray_img = ImageOps.grayscale(img)

    # Apply a binary threshold to isolate black text
    # Using the best threshold found from the previous step (105)
    threshold = 105
    threshold_img = gray_img.point(lambda x: 0 if x < threshold else 255, '1')

    # Invert the image so the text is white on a black background (often helps Tesseract)
    inverted_img = ImageOps.invert(threshold_img)


    # Specify the languages to use (English, Simplified Chinese, Korean, Japanese)
    # 'eng', 'chi-sim', 'kor', 'jpn' are the Tesseract language codes
    languages = 'eng+chi-sim+kor+jpn'

    # Perform OCR
    # Using PSM 3 as it seemed reasonable for this layout
    text_processed_thresholded = pytesseract.image_to_string(inverted_img, lang=languages, config=r'--psm 3')

    print("Extracted Text from Processed Image (after thresholding):")
    print(text_processed_thresholded)

    # Optional: Display the processed image to see the effect of thresholding
    # from IPython.display import display
    # display(inverted_img)


except FileNotFoundError:
    print(f"Error: Image file not found at {image_path}")
except Exception as e:
    print(f"An error occurred: {e}")

"""**Important Notes:**

1.  **Adjust Threshold:** The `threshold` value is crucial and will likely need to be adjusted based on the specific brightness and contrast of your image. Experiment with different values to find what best isolates the black text without picking up too much background noise.
2.  **Experiment with PSM:** After thresholding, the image might have a different appearance, so you might need to experiment with different `psm` values in the `pytesseract.image_to_string` function call to see which one works best on the processed image.
3.  **Display Processed Image:** Uncomment the lines at the end of the code cell (`from IPython.display import display` and `display(inverted_img)`) to see the image after thresholding and inversion. This can help you determine if the threshold is set correctly.

We can iterate through different threshold values and evaluate the OCR output against the expected text to find the optimal threshold for your image.
"""

# Cell ID: 4e00703c
import pytesseract
from PIL import Image, ImageOps
import difflib # Library for comparing sequences

# Replace 'your_image.png' with the path to your image file
image_path = 's27NameOnly.png' # Make sure this is the correct path to your image

# Get the expected text from the markdown cell (replace with the actual content of your markdown cell)
# You'll need to manually copy the content from cell pdArBHB0TLKV and paste it here
expected_text = """[Rac] Riz Maui

Warzone #27

[lolz] 베일 lolz

Warzone #27

[lolz] 도사 lolz

Warzone #27

[x3y] 空降

Warzone #27

[lolz] 폭주핑 lolz

Warzone #27

[Rac] Nada L

Warzone #27

[lolz] 우렁이 lolz

Warzone #27"""

try:
    img = Image.open(image_path).convert('RGB')

    # Convert the image to grayscale
    gray_img = ImageOps.grayscale(img)

    best_threshold = -1
    best_accuracy = 0
    best_extracted_text = ""

    # Iterate through a range of threshold values
    # You might need to adjust the range (e.g., 0 to 255) and step size
    for threshold in range(50, 150, 5): # Example range: from 50 to 145 with step 5
        # Apply a binary threshold
        threshold_img = gray_img.point(lambda x: 0 if x < threshold else 255, '1')

        # Invert the image
        inverted_img = ImageOps.invert(threshold_img)

        # Perform OCR
        languages = 'eng+chi-sim+kor+jpn'
        text_processed = pytesseract.image_to_string(inverted_img, lang=languages, config=r'--psm 3')

        # Calculate accuracy (using a simple ratio of matching characters)
        # A more sophisticated comparison might be needed for complex differences
        matcher = difflib.SequenceMatcher(None, expected_text, text_processed)
        accuracy = matcher.ratio()

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_threshold = threshold
            best_extracted_text = text_processed

    print(f"Best Threshold: {best_threshold}")
    print(f"Best Accuracy: {best_accuracy:.2f}")
    print("Extracted Text with Best Threshold:")
    print(best_extracted_text)

except FileNotFoundError:
    print(f"Error: Image file not found at {image_path}")
except Exception as e:
    print(f"An error occurred: {e}")

"""**Note:**

*   I've included a placeholder for the `expected_text`. **Please manually copy the content from your markdown cell (`pdArBHB0TLKV`) and paste it into the `expected_text` variable in the code above.**
*   The accuracy calculation is a simple character-based ratio. For more nuanced evaluation, you might need a more sophisticated comparison method.
*   You can adjust the `range()` in the `for` loop to explore a different range of threshold values or change the step size.
"""

# Cell ID: bebff107
# Assuming 'text_processed_thresholded' variable contains the output from the previous OCR cell (fce5e8a3)

# Split the extracted text into lines
lines_thresholded = text_processed_thresholded.strip().split('\n')

# Select every other line, starting from the first line (index 0)
# Adjust the starting index or step if the pattern is different
desired_lines_thresholded = lines_thresholded[0::2]

# Join the selected lines back into a single string or process them further
processed_text_thresholded = '\n'.join(desired_lines_thresholded)

print("Processed Text (Every Other Line after Thresholding):")
print(processed_text_thresholded)

"""**Note:** This code assumes a consistent pattern in the output of the thresholded OCR where the desired information is on the first, third, fifth, etc., lines and the unwanted information is on the second, fourth, sixth, etc., lines. If the pattern is different, you might need to adjust the slicing `lines_thresholded[0::2]`."""

# Cell ID: 23994ede
!pip install opencv-python

# Cell ID: ba263ccf
import cv2
import numpy as np
from PIL import Image

# Replace 'your_image.png' with the path to your image file
image_path = 's27NameOnly.png' # Make sure this is the correct path

try:
    # Load the image using OpenCV
    # Use cv2.IMREAD_COLOR to ensure color information is loaded
    img_cv2 = cv2.imread(image_path, cv2.IMREAD_COLOR)

    if img_cv2 is None:
        raise FileNotFoundError(f"Error: Image not loaded. Check if the file exists at {image_path}")

    # Convert to grayscale for easier line detection
    gray_img = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2GRAY)

    # Define a threshold for white lines (adjust as needed)
    # Pixels with intensity above this threshold will be considered white
    white_threshold = 240

    # Find rows that are predominantly white
    # We can check the average pixel intensity or the count of white pixels in each row
    white_line_rows = []
    height, width = gray_img.shape

    for y in range(height):
        # Calculate the percentage of white pixels in the row
        white_pixel_count = np.sum(gray_img[y, :] > white_threshold)
        white_percentage = (white_pixel_count / width) * 100

        # If a row is more than 90% white, consider it a white line
        # You might need to adjust this percentage
        if white_percentage > 90:
            white_line_rows.append(y)

    # Determine the boundaries of each player block based on white lines
    player_blocks = []
    start_row = 0
    for row in white_line_rows:
        # Add a block if there's a significant gap between the last white line and this one
        if row - start_row > 10: # Adjust this value based on expected spacing
            player_blocks.append((start_row, row))
        start_row = row + 1 # Start the next block after the white line

    # Add the last block if the image doesn't end with a white line
    if height - start_row > 10: # Adjust this value
         player_blocks.append((start_row, height))


    print(f"Detected {len(white_line_rows)} white lines at rows: {white_line_rows}")
    print(f"Identified {len(player_blocks)} player blocks.")

    # Now you have the row ranges for each player block in 'player_blocks'
    # The next step would be to crop these blocks and process them further


except FileNotFoundError as e:
    print(e)
except Exception as e:
    print(f"An error occurred: {e}")

"""**Notes:**

*   The `white_threshold` and the percentage check (`white_percentage > 90`) might need to be adjusted based on how "white" the separation lines are in your image.
*   The logic for defining `player_blocks` assumes that the white lines are consistent separators between blocks. You might need to fine-tune the `if row - start_row > 10:` condition based on the spacing in your image.
*   This code only identifies the row ranges for each block. The next steps will involve cropping these blocks and applying further processing or OCR to each one.
"""

# Cell ID: 200c2369
import pytesseract
from PIL import Image
import cv2 # Already imported in the previous cell, but good to be explicit
import numpy as np # Already imported

# Assuming 'img_cv2' from the previous cell is still available
# Assuming 'player_blocks' from the previous cell is still available

try:
    # Convert the OpenCV image back to PIL Image format for cropping with PIL
    # PIL's crop method is often more convenient with bounding box tuples
    img_pil = Image.fromarray(cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB))

    extracted_data = []

    print("Processing each player block:")

    for i, (start_row, end_row) in enumerate(player_blocks):
        print(f"\nProcessing block {i+1} (rows {start_row} to {end_row})")

        # Crop the player block using PIL's crop method
        # The bounding box is (left, upper, right, lower)
        # We crop from the full width of the image
        player_block_img = img_pil.crop((0, start_row, img_pil.width, end_row))

        # --- Apply image processing (optional, but recommended based on previous steps) ---
        # Convert the cropped block to grayscale
        gray_block = player_block_img.convert('L')

        # Apply thresholding (using the best threshold found earlier, 105)
        threshold = 105
        thresholded_block = gray_block.point(lambda x: 0 if x < threshold else 255, '1')

        # Invert the image (white text on black background)
        inverted_block = ImageOps.invert(thresholded_block)
        # --- End of image processing ---


        # Perform OCR on the processed player block
        languages = 'eng+chi-sim+kor+jpn'
        # Using PSM 6 (single uniform block) might be good for these smaller blocks
        # Or PSM 3 (automatic) or PSM 7 (single text line)
        # Experimentation might be needed here
        block_text = pytesseract.image_to_string(inverted_block, lang=languages, config=r'--psm 3') # Switched back to PSM 3


        print(f"Extracted Text from Block {i+1}:")
        print(block_text)

        # You would then parse 'block_text' to extract the clan tag, username, etc.
        # For now, we'll just store the raw extracted text
        extracted_data.append(block_text.strip())

    print("\nFinished processing all blocks.")
    print("\nAll Extracted Data (raw):")
    for i, data in enumerate(extracted_data):
        print(f"Block {i+1}:\n{data}\n---")


except NameError:
    print("Error: 'img_cv2' or 'player_blocks' from the previous cell are not defined. Please run the previous cell first.")
except Exception as e:
    print(f"An error occurred: {e}")

"""**Notes:**

*   This code assumes that `img_cv2` and `player_blocks` variables from the previous cell (`ba263ccf`) are available in the environment.
*   I've included the thresholding and inversion steps within the loop for each cropped block, using the best threshold we found earlier (105). You can adjust this or remove these steps if you find they don't help for individual blocks.
*   You might need to experiment with the `psm` value in `pytesseract.image_to_string` for the individual blocks to get the best results.
*   The `extracted_data` list now contains the raw OCR output for each player block. The next step would be to parse each string in this list to specifically extract the clan tag and username.
"""

# Cell ID: a64b1cb7
# Assuming 'extracted_data' from the previous cell is available

parsed_data = []

print("Parsing extracted data:")

for i, block_text in enumerate(extracted_data):
    print(f"\nProcessing block {i+1}:")
    lines = block_text.strip().split('\n')
    clan_tag = None
    username = None
    found_clan_tag_line = False

    for line in lines:
        # Clean up the line: remove leading/trailing whitespace and potential form feed characters
        cleaned_line = line.strip().replace('\x0c', '')

        # Skip empty lines or lines that are likely just the form feed character
        if not cleaned_line:
            continue

        # Attempt to find the clan tag pattern: text within square brackets
        # Prioritizing lines that start with '[' if that's the expected format
        if cleaned_line.startswith('[') and ']' in cleaned_line:
            tag_start = cleaned_line.find('[') # Should be 0
            tag_end = cleaned_line.find(']')

            if tag_end > tag_start:
                clan_tag = cleaned_line[tag_start + 1 : tag_end].strip()
                # The rest of the line after the clan tag is the potential username
                username = cleaned_line[tag_end + 1 :].strip()
                found_clan_tag_line = True
                # Assuming the first line that starts with '[' and contains ']' is the correct one, break
                break
        # Fallback logic: If no line starting with '[' is found, look for any line with brackets
        elif '[' in cleaned_line and ']' in cleaned_line and not found_clan_tag_line:
             tag_start = cleaned_line.find('[')
             tag_end = cleaned_line.find(']')
             if tag_end > tag_start:
                clan_tag = cleaned_line[tag_start + 1 : tag_end].strip()
                username = cleaned_line[tag_end + 1 :].strip()
                found_clan_tag_line = True
                # Keep searching in case a line starting with '[' appears later (less likely but possible with OCR errors)


    # If a line with a clan tag was found, the username should have been extracted on that line.
    # If no clan tag was found, consider the entire block text as the username as a fallback
    # if it doesn't look like a Warzone line.
    if not found_clan_tag_line:
        # Check if the block text looks like a Warzone line and skip if so
        if "Warzone" in block_text or "#" in block_text:
             print(f"  Block {i+1} likely contains Warzone info, skipping as no clan tag found.")
             continue # Skip this block if it looks like Warzone and no tag was found
        else:
            # If no clan tag found and it doesn't look like Warzone, treat the whole text as username
            username = block_text.strip()


    # Filter out entries where no clan tag or username was found
    if clan_tag is not None or username is not None:
         parsed_data.append({'block': i + 1, 'clan_tag': clan_tag, 'username': username})
    else:
         print(f"  Could not extract data for block {i+1}")


print("\nFinished parsing.")
print("\nParsed Data:")
for entry in parsed_data:
    print(entry)

"""**Notes:**

*   This code assumes that the clan tag is always on a line and enclosed in square brackets `[]`.
*   It takes the text after the closing square bracket as the username on that same line.
*   It includes a basic fallback to treat a line without a clan tag as a potential username, but you might need to refine this logic based on the actual variations in your OCR output.
*   The `parsed_data` list contains dictionaries with the block number, extracted clan tag, and username.
"""

# Cell ID: b485f657
# Assuming 'extracted_data' from the previous cell is available

parsed_data = []

print("Parsing extracted data:")

for i, block_text in enumerate(extracted_data):
    print(f"\nProcessing block {i+1}:")
    lines = block_text.strip().split('\n')
    clan_tag = None
    username = None
    found_clan_tag_line = False

    for line in lines:
        # Clean up the line: remove leading/trailing whitespace and potential form feed characters
        cleaned_line = line.strip().replace('\x0c', '')

        # Skip empty lines or lines that are likely just the form feed character
        if not cleaned_line:
            continue

        # Attempt to find the clan tag pattern: line starts with '[' and contains ']'
        if cleaned_line.startswith('[') and ']' in cleaned_line:
            tag_start = cleaned_line.find('[') # Should be 0 if it starts with '['
            tag_end = cleaned_line.find(']')

            # Ensure the closing bracket is after the opening bracket
            if tag_end > tag_start:
                clan_tag = cleaned_line[tag_start + 1 : tag_end].strip()
                # The rest of the line after the clan tag is the potential username
                username = cleaned_line[tag_end + 1 :].strip()
                found_clan_tag_line = True
                # Assuming the first line that starts with '[' and contains ']' is the correct one, break
                break

    # After checking all lines, if a clan tag line was found, add the extracted data.
    # If no clan tag line was found, this block could not be parsed according to the expected format.
    if found_clan_tag_line:
         parsed_data.append({'block': i + 1, 'clan_tag': clan_tag, 'username': username})
    else:
         # If no line starting with '[' and containing ']' was found,
         # this block likely did not contain the expected clan tag/username format.
         # We can optionally add a placeholder or skip this entry.
         print(f"  Could not find expected clan tag/username format in block {i+1}")
         # Optionally, add a placeholder for blocks that couldn't be parsed
         # parsed_data.append({'block': i + 1, 'clan_tag': None, 'username': None})


print("\nFinished parsing.")
print("\nParsed Data:")
for entry in parsed_data:
    print(entry)

# Cell ID: b1acb4ba
# Assuming 'parsed_data' from the previous parsing cell (b485f657) is available

# Get the expected output from the markdown cell pB5c7GcoeUM5
# You might need to manually copy the content if direct access is not working as expected
expected_output_raw = """[Rac] Riz Maui

[lolz] 베일 lolz

[lolz] 도사 lolz

[x3y] 空降

[lolz] 폭주핑 lolz

[Rac] Nada L

[lolz] 우렁이 lolz
""" # Manually copied content from pB5c7GcoeUM5

# Process the expected output into a list of dictionaries for easier comparison
expected_parsed_data = []
expected_lines = expected_output_raw.strip().split('\n')

for line in expected_lines:
    cleaned_line = line.strip()
    if not cleaned_line:
        continue

    clan_tag = None
    username = cleaned_line # Assume the whole line is the username by default

    # Attempt to find the clan tag pattern
    if '[' in cleaned_line and ']' in cleaned_line:
        tag_start = cleaned_line.find('[')
        tag_end = cleaned_line.find(']')

        if tag_start != -1 and tag_end != -1 and tag_end > tag_start:
            clan_tag = cleaned_line[tag_start + 1 : tag_end].strip()
            username = cleaned_line[tag_end + 1 :].strip()

    expected_parsed_data.append({'clan_tag': clan_tag, 'username': username})

print("Comparing Extracted Data with Expected Data:")
print("-" * 40)

# Compare the extracted data with the expected data
# Note: The block numbers might not align perfectly if some blocks were not parsed
min_len = min(len(parsed_data), len(expected_parsed_data))
max_len = max(len(parsed_data), len(expected_parsed_data))

for i in range(max_len):
    extracted_entry = parsed_data[i] if i < len(parsed_data) else None
    expected_entry = expected_parsed_data[i] if i < len(expected_parsed_data) else None

    print(f"Entry {i + 1}:")
    print(f"  Extracted: {extracted_entry}")
    print(f"  Expected:  {expected_entry}")

    if extracted_entry != expected_entry:
        print("  Mismatch!")
    else:
        print("  Match")
    print("-" * 20)

if len(parsed_data) != len(expected_parsed_data):
    print(f"Warning: Number of extracted entries ({len(parsed_data)}) does not match the number of expected entries ({len(expected_parsed_data)}).")

# Cell ID: 1e8c8857
import cv2
import numpy as np
from PIL import Image
import pytesseract
from PIL import ImageOps
import os # Import the os module for creating directories

# Assuming 'img_cv2' from the previous white line detection cell is still available
# Assuming 'player_blocks' from the previous white line detection cell is still available

try:
    if 'img_cv2' not in locals() or 'player_blocks' not in locals():
         raise NameError("'img_cv2' or 'player_blocks' from the previous cell are not defined. Please run the white line detection cell first.")

    # Convert the OpenCV image back to PIL Image format for processing
    img_pil = Image.fromarray(cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB))

    # Store processed text blocks and their original indices for a second pass
    processed_blocks_info = []

    # Dictionary to store identified clan tags and their processed templates {template_image_tuple: corresponding_text, ...}
    identified_clan_templates = {} # { template_image_tuple: corresponding_text, ... }


    # Create a directory to save the processed images for verification
    output_dir = 'processed_text_blocks'
    os.makedirs(output_dir, exist_ok=True)

    print("--- PASS 1: Identifying potential Clan Tag Templates using pixel comparison ---")

    # Define a threshold for considering a pixel as "black" for initial text block detection
    black_pixel_threshold_detection = 70

    # Define a threshold for isolating black text within the detected block for processing
    black_text_threshold_segmentation = 50

    # Define the width of the region to use for template identification and matching
    clan_tag_template_width = 50 # Based on user feedback

    # Template matching threshold for considering a block a match with an EXISTING template
    match_threshold = 0.8 # Adjust as needed

    # OCR confidence threshold for identifying potential new templates text (for dictionary key/verification)
    new_template_ocr_conf_threshold = 60 # Use a reasonable confidence for text identification

    # First pass: Iterate through blocks to find and store potential templates
    for i, (start_row_block, end_row_block) in enumerate(player_blocks):
        print(f"\nProcessing block {i+1} (rows {start_row_block} to {end_row_block}) in Pass 1")

        block_img_cv2 = img_cv2[start_row_block:end_row_block, 0:img_cv2.shape[1]]

        gray_block = cv2.cvtColor(block_img_cv2, cv2.COLOR_BGR2GRAY)
        height_block, width_block = gray_block.shape

        text_start_row_in_block = -1
        text_end_row_in_block = -1
        found_first_black_row = False

        for r in range(height_block):
            has_black_pixel = np.any(gray_block[r, :] < black_pixel_threshold_detection)
            if has_black_pixel and not found_first_black_row:
                text_start_row_in_block = r
                found_first_black_row = True
            elif not has_black_pixel and found_first_black_row:
                text_end_row_in_block = r
                break

        if text_start_row_in_block != -1 and text_end_row_in_block == -1:
            text_end_row_in_block = height_block

        if text_start_row_in_block != -1 and text_end_row_in_block != -1 and text_end_row_in_block > text_start_row_in_block:
            text_block_img_cv2 = block_img_cv2[text_start_row_in_block:text_end_row_in_block, :]
            detected_text_block_height = text_block_img_cv2.shape[0]

            # Store the original text block image and its index for Pass 2
            processed_blocks_info.append({
                'original_block_index': i,
                'text_block_img_cv2': text_block_img_cv2,
            })

            # Ensure the block is wide enough to extract the template region
            if text_block_img_cv2.shape[1] >= clan_tag_template_width:
                # Process the left region for template identification/matching
                left_region_img_cv2 = text_block_img_cv2[:, 0:clan_tag_template_width]
                gray_left_region = cv2.cvtColor(left_region_img_cv2, cv2.COLOR_BGR2GRAY)
                _, processed_left_region = cv2.threshold(gray_left_region, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)

                # --- Identify Potential New Templates from Non-Matching Blocks ---
                # First, check if this processed_left_region matches any *existing* identified templates
                matched_existing = False
                best_match_score = 0
                matched_template_img_tuple = None

                if identified_clan_templates:
                     print("  Checking against existing templates...")
                     # Convert current processed_left_region to tuple for comparison with template keys
                     processed_left_region_tuple = tuple(map(tuple, processed_left_region.tolist()))

                     # Check if this exact processed region has already been added as a template
                     if processed_left_region_tuple in identified_clan_templates:
                          print("  This left region is already an identified template.")
                          matched_existing = True # Consider it matched if it's already a template
                          # Find its match score against itself (should be 1.0 for TM_CCOEFF_NORMED)
                          # This is a bit redundant but aligns with the logic flow
                          template_img = np.array(processed_left_region_tuple, dtype=np.uint8)
                          if processed_left_region.shape[0] >= template_img.shape[0] and processed_left_region.shape[1] >= template_img.shape[1]:
                               result = cv2.matchTemplate(processed_left_region, template_img, cv2.TM_CCOEFF_NORMED)
                               min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
                               best_match_score = max_val # Should be close to 1.0
                               matched_template_img_tuple = processed_left_region_tuple # Matched itself
                               print(f"    Match score against itself: {best_match_score:.2f}")

                     # If not an exact match as a template, perform sliding window matching against others
                     if not matched_existing:
                         print("  Not an exact match as an existing template. Performing sliding window match...")
                         best_match_score_against_others = 0
                         best_match_template_tuple_against_others = None

                         for template_img_tuple in identified_clan_templates.keys(): # Iterate keys (image tuples)
                              template_img = np.array(template_img_tuple, dtype=np.uint8) # Convert tuple back to numpy array
                              template_h, template_w = template_img.shape

                              if processed_left_region.shape[0] >= template_h and processed_left_region.shape[1] >= template_w:
                                   result = cv2.matchTemplate(processed_left_region, template_img, cv2.TM_CCOEFF_NORMED)
                                   min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
                                   current_match_score = max_val
                                   # print(f"    Match score against existing template: {current_match_score:.2f}") # Too verbose

                                   if current_match_score > best_match_score_against_others:
                                       best_match_score_against_others = current_match_score
                                       best_match_template_tuple_against_others = template_img_tuple

                         # Check if the best match against existing templates is above the threshold
                         if best_match_score_against_others >= match_threshold:
                             print(f"  Block {i+1} matches an existing template with score {best_match_score_against_others:.2f}")
                             matched_existing = True
                             best_match_score = best_match_score_against_others
                             matched_template_img_tuple = best_match_template_tuple_against_others
                         else:
                             print(f"  Block {i+1} does not match any existing template with score >= {match_threshold}. Best score was {best_match_score_against_others:.2f}.")


                # If it did NOT match any existing template with high confidence, consider it a potential NEW template
                # We still need to ensure it's a valid region to be a template (e.g., sufficient height/black pixels)
                if not matched_existing:
                     print(f"  Block {i+1} left region is a potential NEW template.")

                     # Add this processed_left_region as a new template
                     # Use the tuple representation as the key for hashability
                     new_template_tuple = tuple(map(tuple, processed_left_region.tolist()))

                     if new_template_tuple not in identified_clan_templates:
                          # Perform OCR on this region to get its text for identification (optional, but helpful for debugging)
                          # This OCR result is NOT used for final data extraction in Pass 2.
                          languages = 'eng+chi-sim+kor+jpn'
                          temp_ocr_text = "Unknown"
                          try:
                              temp_ocr_data = pytesseract.image_to_data(Image.fromarray(processed_left_region), lang=languages, config=r'--psm 8', output_type=pytesseract.Output.DICT)
                              potential_text = [temp_ocr_data['text'][b].strip() for b in range(len(temp_ocr_data['level'])) if int(temp_ocr_data['conf'][b]) > new_template_ocr_conf_threshold] # Use defined confidence
                              if potential_text:
                                  new_tag_text = " ".join(potential_text)
                                  # Optional: Refine text if it looks like a bracketed tag
                                  if '[' in new_tag_text and ']' in new_tag_text:
                                      start_idx = new_tag_text.find('[')
                                      end_idx = new_tag_text.find(']')
                                      if end_idx > start_idx:
                                          temp_ocr_text = new_tag_text[start_idx:end_idx+1]
                                      else:
                                          temp_ocr_text = new_tag_text # Keep original if brackets are in wrong order
                                  else:
                                      temp_ocr_text = new_tag_text # Keep original if no brackets found
                              else:
                                  temp_ocr_text = "Unknown" # No confident text found
                          except Exception as e:
                              print(f"  Warning: OCR failed during new template text identification: {e}")


                          identified_clan_templates[new_template_tuple] = temp_ocr_text # Store template image tuple and its OCR text

                          print(f"  Added new potential template from block {i+1}. Initial OCR Text: '{temp_ocr_text}'")

                          # Optional: Save the new template image for verification
                          output_path_new_template = os.path.join(output_dir, f'template_block_{i+1}.png')
                          cv2.imwrite(output_path_new_template, cv2.cvtColor(processed_left_region, cv2.COLOR_GRAY2BGR)) # Save as BGR

                     else:
                          print(f"  Potential new template from block {i+1} is already in the list.")

            else:
                 print(f"  Warning: Processed block {i+1} is too narrow ({text_block_img_cv2.shape[1]} < {clan_tag_template_width}). Cannot use for template identification/matching.")


        else:
            # This block was skipped by text block detection
            block_height = end_row_block - start_row_block
            print(f"  Block {i+1} was skipped by text block detection. Original block height: {block_height} rows.")
            # Do not add to processed_blocks_info as it's not a valid text block


    print("\n--- PASS 1 Finished ---")
    print("\nIdentified Potential Clan Tag Templates:")
    if identified_clan_templates:
        for template_tuple, tag_text in identified_clan_templates.items():
            temp_img = np.array(template_tuple, dtype=np.uint8)
            print(f"- Initial OCR Text: '{tag_text}', Dimensions (w, h): ({temp_img.shape[1]}, {temp_img.shape[0]})")
    else:
        print("No clan tag templates were identified in Pass 1.")


    print("\n--- PASS 2: Matching blocks against identified templates and extracting data ---")

    final_extracted_data = []

    # Sort processed_blocks_info by original block index to maintain order
    processed_blocks_info.sort(key=lambda x: x['original_block_index'])

    # Define Tesseract config for username OCR (Adjust this variable to experiment!)
    username_ocr_config = r'--psm 7' # Try different PSM values or add other flags like --oem 1


    # Second pass: Iterate through processed blocks and match against all identified templates
    for block_info in processed_blocks_info:
        i = block_info['original_block_index']
        text_block_img_cv2 = block_info['text_block_img_cv2']
        # Re-process left region in Pass 2 to ensure consistency, or use stored?
        # Let's re-process the left region to be safe
        processed_left_region = None
        if text_block_img_cv2.shape[1] >= clan_tag_template_width:
             gray_left_region = cv2.cvtColor(text_block_img_cv2[:, 0:clan_tag_template_width], cv2.COLOR_BGR2GRAY)
             _, processed_left_region = cv2.threshold(gray_left_region, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)


        print(f"\nProcessing block {i+1} in Pass 2")

        clan_tag_text = "N/A (No Match)" # Default if no template match
        username_text = "N/A (No Match)" # Default if no template match
        overall_confidence = 0 # Default confidence
        matched_a_template = False
        matched_template_text = "None" # Store the OCR text of the matched template
        best_match_template_img_tuple = None
        best_match_score_overall = 0


        if processed_left_region is not None and identified_clan_templates:
             print("  Attempting to match against all identified templates...")

             for template_tuple, tag_text in identified_clan_templates.items():
                  template_img = np.array(template_tuple, dtype=np.uint8) # Convert tuple back to numpy array
                  template_h, template_w = template_img.shape
                  # Ensure search region is large enough vertically for the template height
                  if processed_left_region.shape[0] >= template_h and processed_left_region.shape[1] >= template_w:
                       # Perform template matching (sliding window in height)
                       result = cv2.matchTemplate(processed_left_region, template_img, cv2.TM_CCOEFF_NORMED)
                       min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
                       current_match_score = max_val

                       # If this is the best score so far for this block across all templates
                       if current_match_score > best_match_score_overall:
                           best_match_score_overall = current_match_score
                           best_match_template_img_tuple = template_tuple # Store the template image tuple
                           matched_template_text = tag_text # Store the text from the template dictionary


             # Check if the best match score against ANY template is above the threshold
             print(f"  Best match score for block {i+1} against all templates: {best_match_score_overall:.2f} (Template text: '{matched_template_text}')")
             if best_match_score_overall >= match_threshold:
                 matched_a_template = True
                 overall_confidence = best_match_score_overall * 100
                 print(f"  Block {i+1} matched template '{matched_template_text}'. Proceeding with extraction.")

                 # --- Refined Extraction: Find precise clan tag location and crop username ---
                 # Use the best matching template image to find its precise location within the processed_left_region
                 # Need to ensure best_match_template_img_tuple is not None if best_match_score_overall > threshold
                 if best_match_template_img_tuple is not None:
                     template_img = np.array(best_match_template_img_tuple, dtype=np.uint8)
                     # Perform template matching on the *processed_text_block* (full width) to find the precise location of the template
                     # Need to perform sliding window search across the full width of the processed text block
                     if processed_text_block.shape[0] >= template_img.shape[0] and processed_text_block.shape[1] >= template_img.shape[1]:
                          result_precise = cv2.matchTemplate(processed_text_block, template_img, cv2.TM_CCOEFF_NORMED)
                          min_val_precise, max_val_precise, min_loc_precise, max_loc_precise = cv2.minMaxLoc(result_precise)

                          # The top-left corner of the best match within the processed_text_block
                          clan_tag_start_x_in_block = max_loc_precise[0]
                          clan_tag_start_y_in_block = max_loc_precise[1]
                          clan_tag_end_x_in_block = clan_tag_start_x_in_block + template_img.shape[1] # Use template width
                          clan_tag_end_y_in_block = clan_tag_start_y_in_block + template_img.shape[0] # Use template height

                          print(f"  Precise template match location in block {i+1}: ({clan_tag_start_x_in_block}, {clan_tag_start_y_in_block}) with dimensions ({template_img.shape[1]}, {template_img.shape[0]})")

                          # Define the username region as the area to the right of the matched clan tag
                          # Add a small buffer (e.g., 5 pixels) after the clan tag region
                          username_region_start_x = clan_tag_end_x_in_block + 5
                          username_region_start_y = 0 # Use full height of the text block
                          username_region_end_x = text_block_img_cv2.shape[1]
                          username_region_end_y = text_block_img_cv2.shape[0]

                          # Ensure username region coordinates are valid
                          username_region_start_x = max(0, username_region_start_x)
                          username_region_start_y = max(0, username_region_start_y)
                          username_region_end_x = min(text_block_img_cv2.shape[1], username_region_end_x)
                          username_region_end_y = min(text_block_img_cv2.shape[0], username_region_end_y)


                          # Crop the username region from the original text_block_img_cv2
                          username_img_cv2 = text_block_img_cv2[username_region_start_y:username_region_end_y, username_region_start_x:username_region_end_x]

                          clan_tag_text = matched_template_text # Use the text from the matched template dictionary

                          if username_img_cv2.size > 0: # Check if the cropped username region is valid and not empty
                               print("  Performing OCR on the cropped username region.")
                               # Apply thresholding and inversion to the username region for OCR
                               gray_username = cv2.cvtColor(username_img_cv2, cv2.COLOR_BGR2GRAY)
                               _, thresholded_username = cv2.threshold(gray_username, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)
                               inverted_username = Image.fromarray(thresholded_username)

                               # Save processed username image for verification
                               output_path_username = os.path.join(output_dir, f'block_{i+1}_username_processed.png')
                               inverted_username.save(output_path_username)

                               # Perform OCR on the username region (multi-language, using defined config)
                               languages = 'eng+chi-sim+kor+jpn'
                               username_ocr_data = pytesseract.image_to_data(inverted_username, lang=languages, config=username_ocr_config, output_type=pytesseract.Output.DICT) # Use variable config

                               username_text_elements = [text for text, conf in zip(username_ocr_data['text'], username_ocr_data['conf']) if int(conf) > 0]
                               username_text = ' '.join(username_text_elements).strip()

                          else:
                               print("  Warning: Cropped username region is empty or invalid.")
                               username_text = "N/A (Username Region Empty)"

                     else:
                          print(f"  Warning: Processed block ({processed_text_block.shape[1]}x{processed_text_block.shape[0]}) is too small for best matched template ({template_img.shape[1]}x{template_img.shape[0]}). Cannot perform precise location and extraction.")
                          clan_tag_text = matched_template_text # Still know the tag text
                          username_text = "N/A (Extraction Skipped)" # Indicate extraction failed


             else:
                 print(f"  Block {i+1} did not match any identified template with score >= {match_threshold}.")
                 # Fallback: Perform OCR on the whole block if no template matched
                 print("  Using whole detected text block for OCR (fallback).")
                 # Apply processing to the whole detected text block (using the stored original crop)
                 gray_text_block_fallback = cv2.cvtColor(text_block_img_cv2, cv2.COLOR_BGR2GRAY)
                 _, thresholded_text_block_fallback = cv2.threshold(gray_text_block_fallback, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)
                 inverted_text_block_fallback = Image.fromarray(thresholded_text_block_fallback)

                 languages = 'eng+chi-sim+kor+jpn'
                 fallback_data = pytesseract.image_to_data(inverted_text_block_fallback, lang=languages, config=r'--psm 7', output_type=pytesseract.Output.DICT) # Use PSM 7 for fallback
                 fallback_text_elements = [text for text, conf in zip(fallback_data['text'], fallback_data['conf']) if int(conf) > 0]
                 fallback_confidences = [conf for conf in fallback_data['conf'] if int(conf) > 0]
                 block_text_extracted_fallback = ' '.join(fallback_text_elements).strip()
                 overall_confidence = np.mean([int(conf) for conf in fallback_confidences]) if fallback_confidences else 0

                 clan_tag_text = "N/A (Fallback)"
                 username_text = block_text_extracted_fallback


        else:
             # If processed_left_region was None (block too narrow) or no templates were identified in Pass 1
             print(f"  Skipping matching/extraction for block {i+1}: Processed left region not available or no templates identified.")
             # Perform fallback OCR
             print("  Using whole detected text block for OCR (fallback).")
             gray_text_block_fallback = cv2.cvtColor(text_block_img_cv2, cv2.COLOR_BGR2GRAY)
             _, thresholded_text_block_fallback = cv2.threshold(gray_text_block_fallback, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)
             inverted_text_block_fallback = Image.fromarray(thresholded_text_block_fallback)

             languages = 'eng+chi-sim+kor+jpn'
             fallback_data = pytesseract.image_to_data(inverted_text_block_fallback, lang=languages, config=r'--psm 7', output_type=pytesseract.Output.DICT) # Use PSM 7 for fallback
             fallback_text_elements = [text for text, conf in zip(fallback_data['text'], fallback_data['data']['conf']) if int(conf) > 0] # Fixed typo here
             fallback_confidences = [conf for conf in fallback_data['conf'] if int(conf) > 0]
             block_text_extracted_fallback = ' '.join(fallback_text_elements).strip()
             overall_confidence = np.mean([int(conf) for conf in fallback_confidences]) if fallback_confidences else 0

             clan_tag_text = "N/A (Skipped/Fallback)"
             username_text = block_text_extracted_fallback


        print(f"Extracted Data from Block {i+1}:")
        print(f"Clan Tag: '{clan_tag_text}'")
        print(f"Username: '{username_text}'")
        print(f"Overall Confidence: {overall_confidence:.2f}")

        final_extracted_data.append({'block': i + 1, 'clan_tag': clan_tag_text, 'username': username_text, 'confidence': overall_confidence})


    print("\n--- PASS 2 Finished ---")

    print("\nAll Final Extracted Data:")
    for entry in final_extracted_data:
        print(f"Block {entry['block']}:\nClan Tag: {entry['clan_tag']}\nUsername: {entry['username']}\nConfidence: {entry['confidence']:.2f}\n---")


except NameError as e:
    print(e)
except Exception as e:
    print(f"An error occurred: {e}")

# Assuming 'extracted_data_row_by_row' from the previous cell (1e8c8857) is available

parsed_data_row_by_row = []

print("Parsing extracted data from row-by-row detection:")

for i, entry in enumerate(extracted_data_row_by_row):
    block_number = entry['block']
    block_text = entry['text']

    print(f"\nProcessing block {block_number}:")
    lines = block_text.strip().split('\n')
    clan_tag = None
    username = None
    found_clan_tag_line = False

    for line in lines:
        # Clean up the line: remove leading/trailing whitespace and potential form feed characters
        cleaned_line = line.strip().replace('\x0c', '')

        # Skip empty lines
        if not cleaned_line:
            continue

        # Attempt to find the clan tag pattern: text within square brackets
        # Prioritizing lines that start with '[' if that's the expected format
        if cleaned_line.startswith('[') and ']' in cleaned_line:
            tag_start = cleaned_line.find('[') # Should be 0
            tag_end = cleaned_line.find(']')

            if tag_end > tag_start:
                clan_tag = cleaned_line[tag_start + 1 : tag_end].strip()
                # The rest of the line after the clan tag is the potential username
                username = cleaned_line[tag_end + 1 :].strip()
                found_clan_tag_line = True
                # Assuming the first line that starts with '[' and contains ']' is the correct one, break
                break
        # Fallback logic: If no line starting with '[' is found, look for any line with brackets
        elif '[' in cleaned_line and ']' in cleaned_line and not found_clan_tag_line:
             tag_start = cleaned_line.find('[')
             tag_end = cleaned_line.find(']')
             if tag_end > tag_start:
                clan_tag = cleaned_line[tag_start + 1 : tag_end].strip()
                username = cleaned_line[tag_end + 1 :].strip()
                found_clan_tag_line = True
                # Keep searching in case a line starting with '[' appears later (less likely but possible with OCR errors)


    # If a line with a clan tag was found, the username should have been extracted on that line.
    # If no clan tag was found, consider the entire block text as the username as a fallback
    # if it doesn't look like a Warzone line.
    if not found_clan_tag_line:
        # Check if the block text looks like a Warzone line and skip if so
        if "Warzone" in block_text or "#" in block_text:
             print(f"  Block {block_number} likely contains Warzone info, skipping as no clan tag found.")
             continue # Skip this block if it looks like Warzone and no tag was found
        else:
            # If no clan tag found and it doesn't look like Warzone, treat the whole text as username
            username = block_text.strip()


    # Filter out entries where no clan tag or username was found
    if clan_tag is not None or username is not None:
         parsed_data_row_by_row.append({'block': block_number, 'clan_tag': clan_tag, 'username': username})
    else:
         print(f"  Could not extract data for block {block_number}")


print("\nFinished parsing.")
print("\nParsed Data (Row by Row Detection Output):")
for entry in parsed_data_row_by_row:
    print(entry)

import cv2
import numpy as np
from PIL import Image
import pytesseract
from PIL import ImageOps
import os # Import the os module for creating directories

# Assuming 'img_cv2' from the previous white line detection cell is still available
# Assuming 'player_blocks' from the previous white line detection cell is still available

try:
    if 'img_cv2' not in locals() or 'player_blocks' not in locals():
         raise NameError("'img_cv2' or 'player_blocks' from the previous cell are not defined. Please run the white line detection cell first.")

    # Convert the OpenCV image back to PIL Image format for processing
    img_pil = Image.fromarray(cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB))

    extracted_data_refined = []

    # Create a directory to save the processed images for verification
    output_dir = 'processed_text_blocks'
    os.makedirs(output_dir, exist_ok=True)

    print("Processing each player block by detecting black pixel rows and segmenting using OCR data:")

    # Define a threshold for considering a pixel as "black" for text block detection
    black_pixel_threshold_detection = 70 # Adjusted threshold for initial text block detection

    # Define a threshold for isolating black text within the detected block for character detection
    black_text_threshold_segmentation = 50 # A potentially stricter threshold for finding brackets

    for i, (start_row_block, end_row_block) in enumerate(player_blocks):
        print(f"\nProcessing block {i+1} (rows {start_row_block} to {end_row_block})")

        # Crop the block using OpenCV for processing
        block_img_cv2 = img_cv2[start_row_block:end_row_block, 0:img_img_cv2.shape[1]]

        # Convert the block to grayscale for intensity checks
        gray_block = cv2.cvtColor(block_img_cv2, cv2.COLOR_BGR2GRAY)

        height_block, width_block = gray_block.shape

        text_start_row_in_block = -1
        text_end_row_in_block = -1
        found_first_black_row = False

        # Iterate through rows to find the text block based on black pixels
        for r in range(height_block):
            # Check if the row contains any pixel below the black_pixel_threshold for detection
            has_black_pixel = np.any(gray_block[r, :] < black_pixel_threshold_detection)

            if has_black_pixel and not found_first_black_row:
                text_start_row_in_block = r
                found_first_black_row = True

            elif not has_black_pixel and found_first_black_row:
                text_end_row_in_block = r
                break # Stop processing rows in this block


        # If text block extends to the end of the initial block
        if text_start_row_in_block != -1 and text_end_row_in_block == -1:
            text_end_row_in_block = height_block


        if text_start_row_in_block != -1 and text_end_row_in_block != -1 and text_end_row_in_block > text_start_row_in_block:
            # Crop the text block based on the detected rows
            text_block_img_cv2 = block_img_cv2[text_start_row_in_block:text_end_row_in_block, :]

            # --- Check the height of the detected text block ---
            detected_text_block_height = text_block_img_cv2.shape[0]
            print(f"  Detected text block height: {detected_text_block_height} rows")
            if not (18 <= detected_text_block_height <= 23):
                 print(f"  Warning: Detected text block height ({detected_text_block_height} rows) is outside the expected 18-23 range.")
            # --- End of height check ---

            # Convert the detected text block to grayscale for initial OCR pass
            gray_text_block_initial = cv2.cvtColor(text_block_img_cv2, cv2.COLOR_BGR2GRAY)

            # Apply thresholding for initial OCR pass (might need tuning)
            _, thresholded_text_block_initial = cv2.threshold(gray_text_block_initial, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)
            inverted_text_block_initial = Image.fromarray(thresholded_text_block_initial)


            # --- Perform initial OCR pass to get bounding box data ---
            languages = 'eng+chi-sim+kor+jpn'
            # Use PSM 7 or 6 for the whole text block
            initial_data = pytesseract.image_to_data(inverted_text_block_initial, lang=languages, config=r'--psm 7', output_type=pytesseract.Output.DICT)

            # --- Analyze initial OCR data to find bracket locations ---
            open_bracket_boxes = []
            close_bracket_boxes = []

            n_boxes = len(initial_data['level'])
            for b in range(n_boxes):
                if int(initial_data['conf'][b]) > 0: # Only consider confident detections
                    (x, y, w, h) = (initial_data['left'][b], initial_data['top'][b], initial_data['width'][b], initial_data['height'][b])
                    text = initial_data['text'][b]

                    if '[' in text:
                        # Simple check: if the text contains '[', assume it's an open bracket or part of one
                        # Could refine this to check if text is exactly '['
                        open_bracket_boxes.append({'x': x, 'y': y, 'w': w, 'h': h})
                    if ']' in text:
                        # Simple check for ']'
                        close_bracket_boxes.append({'x': x, 'y': y, 'w': w, 'h': h})

            # Sort found bracket boxes by x-coordinate
            open_bracket_boxes.sort(key=lambda box: box['x'])
            close_bracket_boxes.sort(key=lambda box: box['x'])


            clan_tag_region = None
            username_region = None

            # Define regions based on OCR bracket locations
            if open_bracket_boxes and close_bracket_boxes:
                # Assume the first open bracket box is the start and the last close bracket box is the end
                first_open_box = open_bracket_boxes[0]
                last_close_box = close_bracket_boxes[-1]

                # Define clan tag region from the beginning of the first open bracket's x
                # to the end of the last close bracket's x + its width.
                # Use the full height of the detected text block.
                ct_x1 = first_open_box['x']
                ct_y1 = 0
                ct_x2 = last_close_box['x'] + last_close_box['w']
                ct_y2 = detected_text_block_height

                # Ensure coordinates are within the bounds of the text block
                ct_x1 = max(0, ct_x1)
                ct_y1 = max(0, ct_y1)
                ct_x2 = min(text_block_img_cv2.shape[1], ct_x2)
                ct_y2 = min(text_block_img_cv2.shape[0], ct_y2)
                clan_tag_region = (ct_x1, ct_y1, ct_x2, ct_y2)


                # Define username region from after the last close bracket to the end of the text block
                # Add a small buffer after the bracket (adjust 5)
                un_x1 = last_close_box['x'] + last_close_box['w'] + 5
                un_y1 = 0
                un_x2 = text_block_img_cv2.shape[1]
                un_y2 = detected_text_block_height

                # Ensure coordinates are within the bounds of the text block
                un_x1 = max(0, un_x1)
                un_y1 = max(0, un_y1)
                un_x2 = min(text_block_img_cv2.shape[1], un_x2)
                un_y2 = min(text_block_img_cv2.shape[0], un_y2)
                username_region = (un_x1, un_y1, un_x2, un_y2)

                print(f"  Defined Clan Tag Region (based on OCR): {clan_tag_region}")
                print(f"  Defined Username Region (based on OCR): {username_region}")


            # --- Perform final OCR on segmented regions (if found) or the whole block as fallback ---
            clan_tag_text = ""
            username_text = ""
            overall_confidence = 0

            # Use detected regions if found, otherwise use the whole detected text block as fallback
            if clan_tag_region and username_region:
                print("  Using segmented regions for final OCR.")
                # OCR on clan tag region
                ct_x1, ct_y1, ct_x2, ct_y2 = clan_tag_region
                clan_tag_img_cv2_final = text_block_img_cv2[ct_y1:ct_y2, ct_x1:ct_x2]

                if clan_tag_img_cv2_final.size > 0: # Check if the cropped region is valid
                     # Convert to PIL for OCR and apply threshold/invert
                    gray_ct_final = cv2.cvtColor(clan_tag_img_cv2_final, cv2.COLOR_BGR2GRAY)
                    _, thresh_ct_final = cv2.threshold(gray_ct_final, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)
                    inverted_ct_final = Image.fromarray(thresh_ct_final)

                     # Save processed clan tag image
                    output_path_ct = os.path.join(output_dir, f'block_{i+1}_clantag_processed.png')
                    inverted_ct_final.save(output_path_ct)

                    # Perform OCR on clan tag region (PSM 8 for single word/character, lang='eng' often best for tags)
                    ct_data_final = pytesseract.image_to_data(inverted_ct_final, lang='eng', config=r'--psm 8', output_type=pytesseract.Output.DICT)
                    ct_text_elements_final = [text for text, conf in zip(ct_data_final['text'], ct_data_final['conf']) if int(conf) > 0]
                    ct_confidences_final = [conf for conf in ct_data_final['conf'] if int(conf) > 0]
                    clan_tag_text = ''.join(ct_text_elements_final).strip() # Join without spaces for tags
                    clan_tag_confidence = np.mean([int(conf) for conf in ct_confidences_final]) if ct_confidences_final else 0
                else:
                    print(f"  Warning: Clan tag region for block {i+1} is empty or invalid after cropping.")
                    clan_tag_text = "N/A (Invalid Region)"
                    clan_tag_confidence = 0


                # OCR on username region
                un_x1, un_y1, un_x2, un_y2 = username_region
                username_img_cv2_final = text_block_img_cv2[un_y1:un_y2, un_x1:un_x2]

                if username_img_cv2_final.size > 0: # Check if the cropped region is valid
                     # Convert to PIL for OCR and apply threshold/invert
                    gray_un_final = cv2.cvtColor(username_img_cv2_final, cv2.COLOR_BGR2GRAY)
                    _, thresh_un_final = cv2.threshold(gray_un_final, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)
                    inverted_un_final = Image.fromarray(thresh_un_final)

                    # Save processed username image
                    output_path_un = os.path.join(output_dir, f'block_{i+1}_username_processed.png')
                    inverted_un_final.save(output_path_un)


                    # Perform OCR on username region (multi-language, PSM 7)
                    un_data_final = pytesseract.image_to_data(inverted_un_final, lang=languages, config=r'--psm 7', output_type=pytesseract.Output.DICT)
                    un_text_elements_final = [text for text, conf in zip(un_data_final['text'], un_data_final['conf']) if int(conf) > 0]
                    un_confidences_final = [conf for conf in un_data_final['conf'] if int(conf) > 0]
                    username_text = ' '.join(un_text_elements_final).strip()
                    username_confidence = np.mean([int(conf) for conf in un_confidences_final]) if un_confidences_final else 0

                    # Simple combined confidence (can be refined)
                    overall_confidence = (clan_tag_confidence + username_confidence) / 2 if clan_tag_text != "N/A (Invalid Region)" else username_confidence

                else:
                     print(f"  Warning: Username region for block {i+1} is empty or invalid after cropping.")
                     username_text = "N/A (Invalid Region)"
                     username_confidence = 0
                     overall_confidence = clan_tag_confidence # Overall confidence is just clan tag confidence


            else:
                # Fallback: If bracket detection failed, perform OCR on the whole detected text block
                print("  OCR-based bracket detection failed, using whole detected text block for OCR (fallback).")
                # Apply processing to the whole detected text block (grayscale, threshold, invert)
                gray_text_block_fallback = cv2.cvtColor(text_block_img_cv2, cv2.COLOR_BGR2GRAY)
                _, thresholded_text_block_fallback = cv2.threshold(gray_text_block_fallback, black_text_threshold_segmentation, 255, cv2.THRESH_BINARY_INV)
                inverted_text_block_fallback = Image.fromarray(thresholded_text_block_fallback)
                # Save processed fallback image
                output_path_fallback = os.path.join(output_dir, f'block_{i+1}_fallback_processed.png')
                inverted_text_block_fallback.save(output_path_fallback)

                # Perform OCR on the whole detected text block (multi-language, PSM 7)
                fallback_data = pytesseract.image_to_data(inverted_text_block_fallback, lang=languages, config=r'--psm 7', output_type=pytesseract.Output.DICT)
                fallback_text_elements = [text for text, conf in zip(fallback_data['text'], fallback_data['conf']) if int(conf) > 0]
                fallback_confidences = [conf for conf in fallback_data['conf'] if int(conf) > 0]
                block_text_extracted_fallback = ' '.join(fallback_text_elements).strip()
                overall_confidence = np.mean([int(conf) for conf in fallback_confidences]) if fallback_confidences else 0

                # In fallback, we don't have separate clan tag and username
                clan_tag_text = "N/A (Fallback)"
                username_text = block_text_extracted_fallback


            print(f"Extracted Data from Block {i+1}:")
            print(f"Clan Tag: '{clan_tag_text}'")
            print(f"Username: '{username_text}'")
            print(f"Overall Confidence: {overall_confidence:.2f}")

            # Store the extracted data and confidence
            extracted_data_refined.append({'block': i + 1, 'clan_tag': clan_tag_text, 'username': username_text, 'confidence': overall_confidence})

        else:
            # This block was skipped by text block detection
            block_height = end_row_block - start_row_block
            print(f"  Block {i+1} was skipped by text block detection. Original block height: {block_height} rows.")
            # Optionally, store a placeholder for skipped blocks
            # extracted_data_refined.append({'block': i + 1, 'clan_tag': None, 'username': None, 'confidence': 0})


    print("\nFinished processing all blocks.")
    print("\nAll Refined Extracted Data:")
    for entry in extracted_data_refined:
        print(f"Block {entry['block']}:\nClan Tag: {entry['clan_tag']}\nUsername: {entry['username']}\nConfidence: {entry['confidence']:.2f}\n---")


except NameError as e:
    print(e)
except Exception as e:
    print(f"An error occurred: {e}")

"""# Cell ID: 8c753b99
First, we need to install the `pytesseract` library and Tesseract OCR engine, along with the language data files for the languages you need.

# Cell ID: 670d7eea
Now, you can use `pytesseract` to perform OCR. You'll need to upload your image to Colab or access it from a cloud storage service.

Here's an example using a placeholder for your image file:

# Cell ID: pdArBHB0TLKV
[Rac] Riz Maui

Warzone #27

[lolz] 베일 lolz

Warzone #27

[lolz] 도사 lolz

Warzone #27

[x3y] 空降

Warzone #27

[lolz] 폭주핑 lolz

Warzone #27

[Rac] Nada L

Warzone #27

[lolz] 우렁이 lolz

Warzone #27

# Cell ID: c4bdac22
**To use this code:**

1.  **Upload your image:** In the Colab file browser (folder icon on the left), upload the image file you want to process.
2.  **Update `image_path`:** Change `'your_image.png'` in the code to the actual name of your uploaded image file.
3.  **Run the cells:** Execute the code cells sequentially.

This will print the extracted text from your image. Remember that the accuracy can still depend on the image quality and the clarity of the text.

# Cell ID: pB5c7GcoeUM5
[Rac] Riz Maui

[lolz] 베일 lolz

[lolz] 도사 lolz

[x3y] 空降

[lolz] 폭주핑 lolz

[Rac] Nada L

[lolz] 우렁이 lolz

# Cell ID: 2080996d
**Note:** This code assumes a consistent pattern where the desired information is on the first, third, fifth, etc., lines and the unwanted "Warzone" information is on the second, fourth, sixth, etc., lines. If the pattern is different, you might need to adjust the slicing `lines[0::2]`.

# Cell ID: 05b10767
Based on the image description, we can use color thresholding to isolate the black text (clan tag and username) from the rest of the image before performing OCR.

# Cell ID: f093cad7
**Important Notes:**

1.  **Adjust Threshold:** The `threshold` value is crucial and will likely need to be adjusted based on the specific brightness and contrast of your image. Experiment with different values to find what best isolates the black text without picking up too much background noise.
2.  **Experiment with PSM:** After thresholding, the image might have a different appearance, so you might need to experiment with different `psm` values in the `pytesseract.image_to_string` function call to see which one works best on the processed image.
3.  **Display Processed Image:** Uncomment the lines at the end of the code cell (`from IPython.display import display` and `display(inverted_img)`) to see the image after thresholding and inversion. This can help you determine if the threshold is set correctly.

# Cell ID: be991db3
We can iterate through different threshold values and evaluate the OCR output against the expected text to find the optimal threshold for your image.

# Cell ID: 3a7901ce
**Note:**

*   I've included a placeholder for the `expected_text`. **Please manually copy the content from your markdown cell (`pdArBHB0TLKV`) and paste it into the `expected_text` variable in the code above.**
*   The accuracy calculation is a simple character-based ratio. For more nuanced evaluation, you might need a more sophisticated comparison method.
*   You can adjust the `range()` in the `for` loop to explore a different range of threshold values or change the step size.

# Cell ID: e6abc184
**Note:** This code assumes a consistent pattern in the output of the thresholded OCR where the desired information is on the first, third, fifth, etc., lines and the unwanted information is on the second, fourth, sixth, etc., lines. If the pattern is different, you might need to adjust the slicing `lines_thresholded[0::2]`.

# Cell ID: e2b21b23
**Notes:**

*   The `white_threshold` and the percentage check (`white_percentage > 90`) might need to be adjusted based on how "white" the separation lines are in your image.
*   The logic for defining `player_blocks` assumes that the white lines are consistent separators between blocks. You might need to fine-tune the `if row - start_row > 10:` condition based on the spacing in your image.
*   This code only identifies the row ranges for each block. The next steps will involve cropping these blocks and applying further processing or OCR to each one.

# Cell ID: eab029ef
**Notes:**

*   This code assumes that `img_cv2` and `player_blocks` variables from the previous cell (`ba263ccf`) are available in the environment.
*   I've included the thresholding and inversion steps within the loop for each cropped block, using the best threshold we found earlier (105). You can adjust this or remove these steps if you find they don't help for individual blocks.
*   You might need to experiment with the `psm` value in `pytesseract.image_to_string` for the individual blocks to get the best results.
*   The `extracted_data` list now contains the raw OCR output for each player block. The next step would be to parse each string in this list to specifically extract the clan tag and username.

# Cell ID: e9540212
**Notes:**

*   This code assumes that the clan tag is always on a line and enclosed in square brackets `[]`.
*   It takes the text after the closing square bracket as the username on that same line.
*   It includes a basic fallback to treat a line without a clan tag as a potential username, but you might need to refine this logic based on the actual variations in your OCR output.
*   The `parsed_data` list contains dictionaries with the block number, extracted clan tag, and username.
"""